name: ADK Agent Evaluation

on:
  push:
    branches:
      - master
      - main
  pull_request:
    branches:
      - master
      - main
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * 1'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.11'
  CACHE_KEY_PREFIX: v1

jobs:
  lint-and-typecheck:
    name: Lint and Type Check
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            tests/requirements-test.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ruff mypy
          pip install -r requirements.txt

      - name: Run Ruff linter
        run: |
          ruff check . --output-format=github --select=E,F,W,C,N --ignore=E501
        continue-on-error: false

      - name: Run Ruff formatter check
        run: |
          ruff format --check .
        continue-on-error: true

      - name: Run MyPy type checker
        run: |
          mypy app/ --ignore-missing-imports --no-strict-optional --warn-unused-ignores
        continue-on-error: true

  agent-evaluation:
    name: Agent Evaluation Tests
    runs-on: ubuntu-latest
    needs: lint-and-typecheck
    timeout-minutes: 30

    strategy:
      matrix:
        python-version: ['3.10', '3.11']
      fail-fast: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            tests/requirements-test.txt

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ env.CACHE_KEY_PREFIX }}-pip-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ env.CACHE_KEY_PREFIX }}-pip-${{ runner.os }}-py${{ matrix.python-version }}-
            ${{ env.CACHE_KEY_PREFIX }}-pip-${{ runner.os }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r tests/requirements-test.txt

      - name: Check GCP credentials availability
        id: check_creds
        run: |
          if [ -n "${{ secrets.GCP_CREDENTIALS }}" ]; then
            echo "has_creds=true" >> $GITHUB_OUTPUT
          else
            echo "has_creds=false" >> $GITHUB_OUTPUT
            echo "::warning::GCP credentials not configured. Tests will run in mock mode."
          fi

      - name: Authenticate to Google Cloud
        if: steps.check_creds.outputs.has_creds == 'true' && (github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository)
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Set up Google Cloud SDK
        if: steps.check_creds.outputs.has_creds == 'true' && (github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository)
        uses: google-github-actions/setup-gcloud@v2

      - name: Create test database directory
        run: |
          mkdir -p /tmp/test_db
          chmod 777 /tmp/test_db

      - name: Run evaluation tests
        env:
          PYTHONPATH: ${{ github.workspace }}
          TEST_DB_PATH: /tmp/test_db/test.db
          GOOGLE_CLOUD_PROJECT: ${{ secrets.GCP_PROJECT_ID }}
          GOOGLE_CLOUD_LOCATION: us-central1
          GOOGLE_GENAI_USE_VERTEXAI: "true"
          EVAL_PASS_THRESHOLD: 80
        run: |
          pytest tests/test_eval/ \
            -v \
            --tb=short \
            --junit-xml=test-results-py${{ matrix.python-version }}.xml \
            --maxfail=5 \
            -o junit_family=xunit2 \
            || echo "TESTS_FAILED=true" >> $GITHUB_ENV
        continue-on-error: true

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-py${{ matrix.python-version }}
          path: test-results-py${{ matrix.python-version }}.xml
          retention-days: 30

      - name: Upload evaluation logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: eval-logs-py${{ matrix.python-version }}
          path: |
            tests/test_eval/*.log
            /tmp/test_db/*.db
          retention-days: 7
        continue-on-error: true

      - name: Check pass threshold
        if: env.TESTS_FAILED == 'true'
        run: |
          echo "Evaluation tests failed or did not meet pass threshold"
          exit 1

  report-metrics:
    name: Report Evaluation Metrics
    runs-on: ubuntu-latest
    needs: agent-evaluation
    if: always()
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          path: test-results/

      - name: Install XML parser
        run: |
          pip install pytest junitparser

      - name: Parse test results and generate metrics
        id: metrics
        run: |
          python - <<'PYTHON_SCRIPT'
          import os
          import glob
          from junitparser import JUnitXml
          from pathlib import Path

          results_dir = Path("test-results")
          all_results = []

          for xml_file in results_dir.rglob("*.xml"):
              try:
                  xml = JUnitXml.fromfile(str(xml_file))
                  for suite in xml:
                      for case in suite:
                          result = {
                              "name": case.name,
                              "classname": case.classname,
                              "time": case.time,
                              "passed": case.result is None
                          }
                          all_results.append(result)
              except Exception as e:
                  print(f"Error parsing {xml_file}: {e}")

          total_tests = len(all_results)
          passed_tests = sum(1 for r in all_results if r["passed"])
          failed_tests = total_tests - passed_tests
          pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
          avg_time = sum(r["time"] for r in all_results) / total_tests if total_tests > 0 else 0

          print(f"TOTAL_TESTS={total_tests}")
          print(f"PASSED_TESTS={passed_tests}")
          print(f"FAILED_TESTS={failed_tests}")
          print(f"PASS_RATE={pass_rate:.2f}")
          print(f"AVG_TIME={avg_time:.3f}")

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"total_tests={total_tests}\n")
              f.write(f"passed_tests={passed_tests}\n")
              f.write(f"failed_tests={failed_tests}\n")
              f.write(f"pass_rate={pass_rate:.2f}\n")
              f.write(f"avg_time={avg_time:.3f}\n")

          summary = f"""## ADK Agent Evaluation Results

          | Metric | Value |
          |--------|-------|
          | Total Tests | {total_tests} |
          | Passed | {passed_tests} ✅ |
          | Failed | {failed_tests} ❌ |
          | Pass Rate | {pass_rate:.2f}% |
          | Avg Test Time | {avg_time:.3f}s |

          """

          if pass_rate < 80:
              summary += "\n⚠️ **Warning**: Pass rate below 80% threshold\n"
          else:
              summary += "\n✅ **Success**: All quality gates passed\n"

          with open(os.environ["GITHUB_STEP_SUMMARY"], "a") as f:
              f.write(summary)

          with open("metrics_summary.txt", "w") as f:
              f.write(summary)
          PYTHON_SCRIPT

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('metrics_summary.txt', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

      - name: Upload metrics report
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-metrics
          path: metrics_summary.txt
          retention-days: 90

      - name: Check quality gate
        if: needs.agent-evaluation.result == 'failure'
        run: |
          echo "Agent evaluation tests failed. Please review the test results."
          exit 1

  integration-validation:
    name: ADK Integration Validation
    runs-on: ubuntu-latest
    needs: lint-and-typecheck
    if: github.event_name == 'push' || github.event_name == 'schedule'
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r tests/requirements-test.txt

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}
        continue-on-error: true

      - name: Run ADK integration tests
        env:
          PYTHONPATH: ${{ github.workspace }}
          GOOGLE_CLOUD_PROJECT: ${{ secrets.GCP_PROJECT_ID }}
          GOOGLE_CLOUD_LOCATION: us-central1
          GOOGLE_GENAI_USE_VERTEXAI: "true"
        run: |
          pytest tests/integration/test_real_adk_execution.py \
            tests/test_monitoring/test_adk_observability.py \
            tests/test_services/test_adk_*.py \
            -v \
            --tb=short \
            --junit-xml=integration-results.xml \
            -o junit_family=xunit2
        continue-on-error: true

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: integration-results.xml
          retention-days: 30
